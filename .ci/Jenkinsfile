#!/usr/bin/env groovy

@Library('apm@current') _

pipeline {
  agent { label 'ubuntu-20 && immutable' }
  environment {
    BRANCH_NAME_LOWER_CASE = "${env.BRANCH_NAME.toLowerCase().replaceAll('[^a-z0-9-]', '-')}"
    CREATED_DATE = "${new Date().getTime()}"
    ENVIRONMENT = "ci"
    REPO = 'integrations'
    REPO_BUILD_TAG = "${env.REPO}/${env.BUILD_TAG}"
    BASE_DIR = "src/github.com/elastic/${REPO}"
    AWS_ACCOUNT_SECRET = "secret/observability-team/ci/elastic-observability-aws-account-auth"
    HOME = "${env.WORKSPACE}"
    DOCKER_COMPOSE_VERSION = "v2.17.2"
    KIND_VERSION = "v0.17.0"
    K8S_VERSION = "v1.26.0"
    JOB_GCS_BUCKET = 'fleet-ci-temp'
    JOB_GCS_BUCKET_INTERNAL = 'fleet-ci-temp-internal'
    JOB_GCS_CREDENTIALS = 'fleet-ci-gcs-plugin'
    JOB_GCS_EXT_CREDENTIALS = 'fleet-ci-gcs-plugin-file-credentials'
    STACK_VERSION = "${params.stackVersion}"
    BENCHMARK_THRESHOLD = '15'
    // Elastic package links definitions
    ELASTIC_PACKAGE_LINKS_FILE_PATH = "${env.HOME}/${env.BASE_DIR}/links_table.yml"

    // Signing
    JOB_SIGNING_CREDENTIALS = 'sign-artifacts-with-gpg-job'
    INFRA_SIGNING_BUCKET_NAME = 'internal-ci-artifacts'
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER = "${env.REPO_BUILD_TAG}/signed-artifacts"
    INFRA_SIGNING_BUCKET_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.REPO_BUILD_TAG}"
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}"

    // Publishing
    INTERNAL_CI_JOB_GCS_CREDENTIALS = 'internal-ci-gcs-plugin'
    PACKAGE_STORAGE_UPLOADER_CREDENTIALS = 'upload-package-to-package-storage'
    PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT = 'secret/gce/elastic-bekitzur/service-account/package-storage-uploader'
    PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH = "gs://elastic-bekitzur-package-storage-internal/queue-publishing/${env.REPO_BUILD_TAG}"

    INTEGRATIONS_TESTED = "0"
    FORCE_CHECK_ALL = "${params.force_check_all}"
    SKIP_PUBLISHING = "${params.skip_publishing}"
  }
  options {
    timeout(time: 4, unit: 'HOURS')
    buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '20', daysToKeepStr: '30'))
    timestamps()
    ansiColor('xterm')
    disableResume()
    durabilityHint('PERFORMANCE_OPTIMIZED')
    rateLimitBuilds(throttle: [count: 60, durationName: 'hour', userBoost: true])
    quietPeriod(10)
  }
  triggers {
    issueCommentTrigger("(${obltGitHubComments()}|^/test benchmark fullreport)")
  }
  parameters {
    string(name: 'stackVersion', defaultValue: '', description: 'Version of the stack to use for testing.')
    booleanParam(name: 'force_check_all', defaultValue: false, description: 'Force checking all integrations.')
    booleanParam(name: 'skip_publishing', defaultValue: false, description: 'Skip package storage publishing.')
  }
  stages {
    stage('Prepare workspace') {
      steps {
        deleteDir()
        gitCheckout(basedir: "${BASE_DIR}")
        stashV2(name: 'source', bucket: "${JOB_GCS_BUCKET}", credentialsId: "${JOB_GCS_CREDENTIALS}")
      }
    }
    stage('Check Go sources') {
      steps {
        dir("${BASE_DIR}"){
          withMageEnv(){
            sh(label: 'Checks and builds Go sources', script: 'mage -debug check')
            checkGitDiff()
          }
        }
      }
    }
    stage('Publish to Package Storage v2') {
      steps {
        packageStoragePublish()
      }
    }
    stage('Check integrations') {
      steps {
        script {
          if (!fileExists("${BASE_DIR}/packages")) {
            error "Has the stage been restarted? This pipeline requires to be restarted running all stages."
          }
          dir("${BASE_DIR}/packages") {
            def integrations = [:]
            // Include hack to skip temporary files with "@tmp" suffix.
            // For reference: https://issues.jenkins.io/browse/JENKINS-52750
            findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
              if (isPrAffected(it)) {
                integrations[it] = {
                  withNode(labels: 'ubuntu-20 && immutable', sleepMin: 10, sleepMax: 100) {
                    stage("${it}: check") {
                      deleteDir()
                      unstashV2(name: 'source', bucket: "${JOB_GCS_BUCKET}", credentialsId: "${JOB_GCS_CREDENTIALS}")
                      useElasticPackage()
                      try {
                        dir("${BASE_DIR}/packages/${it}") {
                          // https://stackoverflow.com/questions/53541489/updating-environment-global-variable-in-jenkins-pipeline-from-the-stage-level
                          script {
                            INTEGRATIONS_TESTED = '1'
                          }
                          sh(label: "Check integration: ${it}", script: '../../build/elastic-package check -v')
                          withKubernetes() {
                            withSecretVault(secret: "${AWS_ACCOUNT_SECRET}", data: ['access_key': 'AWS_ACCESS_KEY_ID', 'secret_key': 'AWS_SECRET_ACCESS_KEY']) {

                              prepareStack()

                              sh(label: "Test integration: ${it}", script: '''
                                eval "$(../../build/elastic-package stack shellinit)"
                                ../../build/elastic-package test -v --report-format xUnit --report-output file --test-coverage
                                ''')
                              sh(label: "Benchmark integration: ${it}", script: '''
                                eval "$(../../build/elastic-package stack shellinit)"
                                ../../build/elastic-package benchmark pipeline -v --report-format json --report-output file
                                ''')
                            }
                          }
                        }
                      } finally {
                        dir("${BASE_DIR}") {
                          // use docker-compose version installed in prepareStack
                          withEnv(["PATH=${env.WORKSPACE}/bin:${env.PATH}"]) {
                            archiveArtifacts(allowEmptyArchive: true, artifacts: 'build/test-results/*.xml')
                            junit(allowEmptyResults: true, keepLongStdio: true, testResults: "build/test-results/*.xml")
                            stashBenchmarkResults()
                            sh(label: "Collect Elastic stack logs", script: "build/elastic-package stack dump -v --output build/elastic-stack-dump/${it}")
                            archiveArtifacts(allowEmptyArchive: true, artifacts: "build/elastic-stack-dump/${it}/logs/*.log, build/elastic-stack-dump/${it}/logs/fleet-server-internal/**/*")
                            archiveArtifactsSafe("insecure-logs/${it}/elastic-agent-logs/", "build/elastic-stack-dump/${it}/logs/elastic-agent-internal/*.*")
                            archiveArtifactsSafe("insecure-logs/${it}/elastic-agent-logs/default/", "build/elastic-stack-dump/${it}/logs/elastic-agent-internal/default/*")
                            archiveArtifactsSafe("insecure-logs/${it}/container-logs", "build/container-logs/*.log")
                            sh(label: "Take down the Elastic stack", script: 'build/elastic-package stack down -v')
                            stashCoverageReport()
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            parallel integrations
          }
        }
      }
      post {
        always {
          catchError(message: 'gitHubCommentWithBenchmarkReport failed', buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
            gitHubCommentWithBenchmarkReport()
          }
        }
      }
    }
  }
  post {
    always {
      publishCoverageReports()
    }
    cleanup {
      notifyBuildResult(prComment: true)
    }
  }
}

def gitHubCommentWithBenchmarkReport() {
  if ("${INTEGRATIONS_TESTED}" == "0") {
    log(level: 'INFO', text: 'gitHubCommentWithBenchmarkReport: no integration has been tested')
    return
  }
  // report only on a PR basis for obvious reasons (github comments)
  if (isPR()) {
    def benchmarkGitHubFile = 'report.md'
    def benchmarkResults = 'benchmark-results'
    def currentBenchmarkResults = "build/${benchmarkResults}"
    def baseline = "build/${env.CHANGE_TARGET}/${benchmarkResults}"
    def isFullReport = "false"
    if (env.GITHUB_COMMENT?.contains('benchmark fullreport')) {
      isFullReport = "true"
    }
    dir("${BASE_DIR}") {
      // download artifacts for this PR
      def bucketUri = getBenchmarkBucketURI() + "*.xml"
      log(level: 'INFO', text: "googleStorageDownload(bucketUri: ${bucketUri}, localDirectory: ${currentBenchmarkResults}, pathPrefix: ${getBenchmarkPathPrefix()})")
      googleStorageDownload(bucketUri: bucketUri, credentialsId: "${JOB_GCS_CREDENTIALS}", localDirectory: currentBenchmarkResults, pathPrefix: getBenchmarkPathPrefix())

      // download artifacts from the target branch if any
      if (env.CHANGE_TARGET) {
        try {
          copyArtifacts(filter: "${currentBenchmarkResults}/*.xml", flatten: true, optional: true, projectName: getBaselineJobName(), selector: lastWithArtifacts(), target: baseline)
        } catch(e) {
          log(level: 'INFO', text: 'gitHubCommentWithBenchmarkReport: it was not possible to copy the previous build.')
          return
        }
      }

      // for debugging purposes
      if (fileExists(currentBenchmarkResults)) {
        sh(label: 'debug current benchmark', script: "ls -l ${currentBenchmarkResults}", returnStatus: true)
      }
      if (fileExists(baseline)) {
        sh(label: 'debug baseline benchmark', script: "ls -l ${baseline}", returnStatus: true)
      }

      // run the benchmark diff generator
      sh(label: 'elastic-package report benchmark', script: "elastic-package report benchmark --fail-on-missing=false --new=\"${currentBenchmarkResults}\" --old=\"${baseline}\" --threshold=${env.BENCHMARK_THRESHOLD} --report-output-path=\"${benchmarkGitHubFile}\" --full=${isFullReport}")

      // report back with a github comment the benchmark diff
      try {
        if (fileExists("${benchmarkGitHubFile}")) {
          // TODO: existing githubPrComment signature was not done to comment based on a file, but of a string.
          //       hence commentFile needs to point to an unexisting file.
          githubPrComment(message: readFile(benchmarkGitHubFile), commentFile: 'benchmark-report.md')
        }
      } catch(e) {
        log(level: 'INFO', text: "gitHubCommentWithBenchmarkReport: it was not possible to send the message. ${e.toString()}")
      }
    }
  }
}

def getBaselineJobName() {
  // For a Jenkins Multibranch Pipeline, it's possible to know the current
  // full job name with the env variable JOB_NAME.
  // Let's get the full job name for the target branch, in other words
  // the job name for the branch where the PR will be merged to.
  return env.JOB_NAME.replace(env.JOB_BASE_NAME, env.CHANGE_TARGET)
}

def cleanup(){
  dir("${BASE_DIR}"){
    deleteDir()
  }
}

def useElasticPackage() {
  dir("${BASE_DIR}") {
    withGoEnv() {
      sh(label: 'Install elastic-package', script: 'go build -o build/elastic-package github.com/elastic/elastic-package')
    }
  }
}

def useDockerComposeV2() {
  dir("${BASE_DIR}") {
    retryWithSleep(retries: 3) {
      sh(label: "Install docker-compose ${env.DOCKER_COMPOSE_VERSION}", script: """
      mkdir -p ${env.WORKSPACE}/bin
      curl -SL -o ${env.WORKSPACE}/bin/docker-compose "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-linux-x86_64"
      chmod u+x ${env.WORKSPACE}/bin/docker-compose
      ${env.WORKSPACE}/bin/docker-compose version
      """)
    }
  }
}

// Check if there are non-versioned local changes.
// For reference: https://stackoverflow.com/questions/34807971/why-does-git-diff-index-head-result-change-for-touched-files-after-git-diff-or-g
def checkGitDiff() {
  dir("${BASE_DIR}") {
    sh(label: "git update-index", script: 'git update-index --refresh')
    sh(label: "git diff-index", script: 'git diff-index --exit-code HEAD --')
  }
}

def isPrAffected(integrationName) {
  def manifest = readYaml(file: "${integrationName}/manifest.yml")

  // Packages supported in Kibana >= 8.0.0, shouldn't be included in daily tests of the stack 7.x
  if (manifest != null && manifest['conditions'] != null) {
    def kibanaVersionCondition = kibanaVersionConditionFromManifest(manifest)

    if (kibanaVersionCondition != null) {
      if (!kibanaVersionCondition.contains('^7.') && "${env.STACK_VERSION}".startsWith('7.')) {
        echo "[${integrationName}] PR is not affected: unsupported stack (7.x)"
        return false
      }
      if (!kibanaVersionCondition.contains('^8.') && "${env.STACK_VERSION}".startsWith('8.')) {
        echo "[${integrationName}] PR is not affected: unsupported stack (8.x)"
        return false
      }
    }
  }

  if (env.FORCE_CHECK_ALL == "true") {
    echo "[${integrationName}] PR is affected: \"force_check_all\" parameter enabled"
    return true
  }

  // Setting default values for a PR.
  // Source: https://github.com/elastic/apm-pipeline-library/blob/721115cf0fdb2b5a4e1cb6a576bfbb7035d14485/vars/getGitMatchingGroup.groovy#L40-L41
  def from = env.CHANGE_TARGET?.trim() ? "origin/${env.CHANGE_TARGET}" : "${env.GIT_PREVIOUS_COMMIT?.trim() ? env.GIT_PREVIOUS_COMMIT : env.GIT_BASE_COMMIT}"
  def to = env.GIT_BASE_COMMIT

  // If running for an integration branch (main, backport-*) check with
  // GIT_PREVIOUS_SUCCESSFUL_COMMIT to check if the branch is still healthy.
  // If this value is not available, check with last commit.
  if (env.BRANCH_NAME == 'main' || env.BRANCH_NAME.startsWith('backport-')) {
    echo "[${integrationName}] PR is affected: running on ${env.BRANCH_NAME} branch"
    from = env.GIT_PREVIOUS_SUCCESSFUL_COMMIT?.trim() ? env.GIT_PREVIOUS_SUCCESSFUL_COMMIT : "origin/${env.BRANCH_NAME}^"
    to = "origin/${env.BRANCH_NAME}"
  }

  def r = sh(label: "[${integrationName}] git-diff: check non-package files", script: "git diff --name-only \$(git merge-base ${from} ${to}) ${to} | egrep -v '^(packages/|.github/CODEOWNERS)'", returnStatus: true)
  if (r == 0) {
    echo "[${integrationName}] PR is affected: found non-package files"
    return true
  }

  r = sh(label: "[${integrationName}] git-diff: check package files", script: "git diff --name-only \$(git merge-base ${from} ${to}) ${to} | egrep '^packages/${integrationName}/'", returnStatus: true)
  if (r == 0) {
    echo "[${integrationName}] PR is affected: found package files"
    return true
  }

  echo "[${integrationName}] PR is not affected"
  return false
}

def kibanaVersionConditionFromManifest(manifest) {
  def flattened = flattenMap(map: manifest, separator: ".")
  return flattened["conditions.kibana.version"]
}

def withKubernetes(Closure body) {
    def r = sh(label: "git-diff: check if Kubernetes service deployer is used",
               script: "find . -type d | egrep '_dev/deploy/k8s\$'",
               returnStatus: true)
    withEnv(["PATH=${env.WORKSPACE}/bin:${env.PATH}"]) {
      if (r == 0) {
        retryWithSleep(retries: 2, seconds: 5, backoff: true) { sh(label: "Install kind", script: '''
          mkdir -p ${HOME}/bin
          curl -sSLo ${HOME}/bin/kind "https://github.com/kubernetes-sigs/kind/releases/download/${KIND_VERSION}/kind-linux-amd64"
          chmod +x ${HOME}/bin/kind
          kind version
          ''') }
        retryWithSleep(retries: 2, seconds: 5, backoff: true) { sh(label: "Install kubectl", script: '''
          mkdir -p ${HOME}/bin
          curl -sSLo ${HOME}/bin/kubectl "https://storage.googleapis.com/kubernetes-release/release/${K8S_VERSION}/bin/linux/amd64/kubectl"
          chmod +x ${HOME}/bin/kubectl
          kubectl version --client
          ''') }
        sh(label: "Create kind cluster", script: 'kind create cluster --config ../../kind-config.yaml --image kindest/node:${K8S_VERSION}')
      }

      try {
        body()
      } catch(error) {
        throw error // rethrow error up if there is any, but delete k8s cluster first
      } finally {
        if (r == 0) {
          sh(label: "Delete kind cluster", script: 'kind delete cluster || true')
        }
      }
    }
}

def prepareStack() {
  def stackArgs = '-v'

  if ( env.STACK_VERSION?.trim() ) {
    stackArgs += " --version ${env.STACK_VERSION}"
  } else {
    def manifest = readYaml(file: "manifest.yml")
    def kibanaVersionCondition = null

    if (manifest != null && manifest["conditions"] != null) {
      kibanaVersionCondition = kibanaVersionConditionFromManifest(manifest)

      if (kibanaVersionCondition != null) {
        def version = findOldestSupportedVersion(versionCondition: kibanaVersionCondition)
        stackArgs += " --version ${version}"
      }
    }
  }

  // withKubernetes already adds the ${env.WORKSPACE}/bin path to the env. var PATH
  useDockerComposeV2()

  sh(label: "Show docker-compose version", script: 'docker-compose version')

  sh(label: "Update the Elastic stack", script: "../../build/elastic-package stack update ${stackArgs}")
  sh(label: "Boot up the Elastic stack", script: "../../build/elastic-package stack up -d ${stackArgs}")
}

def stashBenchmarkResults() {
  def wildcard = 'build/benchmark-results/*.xml'
  r = sh(label: "isBenchmarkResultsPresent", script: "ls ${wildcard}", returnStatus: true)
  if (r != 0) {
    echo "isBenchmarkResultsPresent: benchmark files not found, report won't be stashed"
    return
  }
  archiveArtifacts(allowEmptyArchive: true, artifacts: wildcard)
  googleStorageUploadExt(bucket: getBenchmarkBucketURI(), credentialsId: "${JOB_GCS_EXT_CREDENTIALS}", pattern: "${wildcard}")
}

def stashCoverageReport() {
  def wildcard = 'build/test-coverage/*.xml'
  r = sh(label: "isCoverageReportPresent", script: "ls ${wildcard}", returnStatus: true)
  if (r != 0) {
    echo "isCoverageReportPresent: coverage files not found, report won't be stashed"
    return
  }

  googleStorageUploadExt(bucket: getCoverageBucketURI(), credentialsId: "${JOB_GCS_EXT_CREDENTIALS}", pattern: "${wildcard}")
}

def publishCoverageReports() {
  stage('Publish coverage reports') {
    if ("${INTEGRATIONS_TESTED}" == "0") {
      log(level: 'INFO', text: 'publichCoverageReports: no integration has been tested')
      return
    }
    dir("${BASE_DIR}") {
      def bucketUri = getCoverageBucketURI() + "*.xml"
      googleStorageDownload(bucketUri: bucketUri, credentialsId: "${JOB_GCS_CREDENTIALS}", localDirectory: 'build/test-coverage', pathPrefix: getCoveragePathPrefix())
      coverageReport('build/test-coverage')
    }
  }
}

def getBenchmarkBucketURI() {
  return "gs://${JOB_GCS_BUCKET}/" + getBenchmarkPathPrefix()
}

def getBenchmarkPathPrefix() {
  return "${env.JOB_NAME}-${env.BUILD_ID}/benchmark-results/"
}

def getCoverageBucketURI() {
  return "gs://${JOB_GCS_BUCKET}/" + getCoveragePathPrefix()
}

def getCoveragePathPrefix() {
  return "${env.JOB_NAME}-${env.BUILD_ID}/test-coverage/"
}

def archiveArtifactsSafe(remotePath, artifacts) {
  r = sh(label: "areArtifactsPresent", script: "ls ${artifacts}", returnStatus: true)
  if (r != 0) {
    echo "areArtifactsPresent: artifacts files not found, nothing will be archived"
    return
  }

  googleStorageUploadExt(
    bucket: "gs://${JOB_GCS_BUCKET_INTERNAL}/${env.JOB_NAME}-${env.BUILD_ID}/${remotePath}",
    credentialsId: "${JOB_GCS_EXT_CREDENTIALS}",
    pattern: artifacts)
}

def packageStoragePublish() {
  if (env.SKIP_PUBLISHING == "true") {
    echo "packageStoragePublish: skipping because skip_publishing param is ${env.SKIP_PUBLISHING}"
    return
  }
  if (env.BRANCH_NAME != 'main' && !env.BRANCH_NAME.startsWith('backport-')) {
    echo "packageStoragePublish: not the main branch or a backport branch, nothing will be published"
    return
  }

  deleteDir()
  unstashV2(name: 'source', bucket: "${JOB_GCS_BUCKET}", credentialsId: "${JOB_GCS_CREDENTIALS}")
  useElasticPackage()

  // Build only unpublished files (need to read version from manifest.yml)
  def unpublishedPresent = false
  dir("${BASE_DIR}/packages") {
    findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
      dir("${it}") {
        def manifest = readYaml(file: "manifest.yml")
        if (manifest != null && manifest["version"] != null && manifest["name"] != null) {
          // package name is retrieved from the manifest in case name it differs from folder name
          def packageZip = manifest["name"] + "-" + manifest["version"] + ".zip"
          if (isAlreadyPublished(packageZip)) {
            return
          }
        }

        withEnv(["PATH=${env.WORKSPACE}/${BASE_DIR}/build:${env.PATH}"]) {
          catchError(message: 'Build package ${it} failed', buildResult: 'SUCCESS', stageResult: 'UNSTABLE') {
            sh(label: "Build integration as zip: ${it}", script: """
              elastic-package check
              elastic-package build --zip
              """)
          }
        }
        unpublishedPresent = true
      }
    }
  }
  if (!unpublishedPresent) {
    echo 'All packages are in sync'
    return
  }

  // Sign unpublished packages
  dir("${BASE_DIR}/build/packages") {
    googleStorageUpload(bucket: "${env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH}",
      credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
      pattern: '*.zip',
      sharedPublicly: false,
      showInline: true)
    withCredentials([string(credentialsId: env.JOB_SIGNING_CREDENTIALS, variable: 'TOKEN')]) {
      triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
        job: 'https://internal-ci.elastic.co/job/elastic+unified-release+master+sign-artifacts-with-gpg',
        token: TOKEN,
        parameters: [
          gcs_input_path: env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH,
        ],
        useCrumbCache: false,
        useJobInfoCache: false)
    }
    googleStorageDownload(bucketUri: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH}/*",
      credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
      localDirectory: '.',
      pathPrefix: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}")
      sh(label: 'Rename .asc to .sig', script: 'for f in *.asc; do mv "$f" "${f%.asc}.sig"; done')
  }

  // Publish packages with signatures. Only packages with generated signatures haven't been published yet.
  withGCPEnv(secret: env.PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT) {
    withCredentials([string(credentialsId: env.PACKAGE_STORAGE_UPLOADER_CREDENTIALS, variable: 'TOKEN')]) {
      dir("${BASE_DIR}/build/packages") {
        findFiles()?.findAll{ it.name.endsWith('.zip') }?.collect{ it.name }?.sort()?.each {
          def packageZip = it
          def packageZipSig = packageZip + ".sig"
          sh(label: 'Upload package .zip file', script: "gsutil cp ${packageZip} ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")
          sh(label: 'Upload package .sig file', script: "gsutil cp ${packageZipSig} ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")
          triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
            job: 'https://internal-ci.elastic.co/job/package_storage/job/publishing-job-remote',
            token: TOKEN,
            parameters: [
              dry_run: false,
              gs_package_build_zip_path: "${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/${packageZip}",
              gs_package_signature_path: "${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/${packageZipSig}"
            ],
            useCrumbCache: true,
            useJobInfoCache: true)
        }
      }
    }
  }
}

def isAlreadyPublished(packageZip) {
  def responseCode = httpRequest(method: "HEAD",
    url: "https://package-storage.elastic.co/artifacts/packages/${packageZip}",
    response_code_only: true)
  if (responseCode != 200) {
    echo "Package Registry HTTP status code for ${packageZip}: ${responseCode}"
  }
  return responseCode == 200
}
