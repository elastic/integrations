config_version: 2
interval: {{interval}}
resource.tracer:
  enabled: {{enable_request_tracer}}
  filename: "../../logs/cel/http-request-trace-*.ndjson"
  maxbackups: 5
{{#if proxy_url}}
resource.proxy_url: {{proxy_url}}
{{/if}}
{{#if ssl}}
resource.ssl: {{ssl}}
{{/if}}
{{#if http_client_timeout}}
resource.timeout: {{http_client_timeout}}
{{/if}}
auth.oauth2:
  provider: google
  google:
    jwt_file: {{jwt_file}}
    jwt_json: {{jwt_json}}
  scopes:
    - https://www.googleapis.com/auth/bigquery.readonly
resource.url: {{bigquery_endpoint}}
state:
  initial_interval: {{initial_interval}}
  lag_time: {{lag_time}}
  batch_size: {{batch_size}}
  project_id: {{project_id}}
  dataset_name: {{dataset_name}}
redact:
  fields: ~
program: |
  (
    state.?want_more.orValue(false) ?
      state
    :
      state.with({
        "start_time": state.?cursor.last_timestamp.orValue(
          timestamp(
            (now - duration(state.initial_interval) - duration(state.lag_time))
          ).as(ts,
            int(ts)*1000000+int(ts-timestamp(int(ts)))
          )
        ),
        "end_time": timestamp(
          (now - duration(state.lag_time))
        ).as(ts,
          int(ts)*1000000+int(ts-timestamp(int(ts)))
        ),
      })
  ).as(state, state.with(
    has(state.next) && state.next.?job_id.hasValue() ? state :
      // Insert a query job to BigQuery.
      // The response for this request includes a unique job ID.
      // Job ID will be used to retrieve the query results.
      post_request(
        state.url.trim_right("/") + "/bigquery/v2/projects/" + state.project_id + "/jobs", "application/json", {
          "configuration": { "query": {
            "useLegacySql": false,
            "query": "SELECT * FROM `" + string(state.project_id) + "." + string(state.dataset_name) + ".daily_*`" +
            " WHERE event_info.timestamp_usec > " + string(state.start_time) +
            " AND event_info.timestamp_usec < " + string(state.end_time) +
            " AND _TABLE_SUFFIX >= '" + timestamp(int(state.start_time)/1000000).format("20060102") + "'" +
            " ORDER BY event_info.timestamp_usec ASC",
          }}
        }.encode_json()
      ).do_request().as(resp, resp.StatusCode == 200 ?
        resp.Body.decode_json().as(body,
          has(body.jobReference) ?
            {"next": {"job_id": string(body.jobReference.jobId)}}
          :
            {}
        )
      :
        {
          "events": {
            "error": {
              "code": string(resp.StatusCode),
              "id": string(resp.Status),
              "message": "POST " + state.url.trim_right("/") + "/bigquery/v2/projects/" + state.project_id + "/jobs: " + (
                size(resp.Body) != 0 ?
                  string(resp.Body)
                :
                  string(resp.Status) + ' (' + string(resp.StatusCode) + ')'
              ),
            },
          },
          "want_more": false,
        }
      )
  )).as(state, state.with(
    has(state.next) && state.next.?job_id.hasValue() ?
      request(
        "GET",
        state.url.trim_right("/") + "/bigquery/v2/projects/"+ state.project_id +"/queries/" + state.next.job_id + "?" + {
          "maxResults": [string(state.batch_size)],
          ?"pageToken": state.?next.page_token.optMap(v, [v]),
        }.format_query()
      ).do_request().as(resp, resp.StatusCode == 200 ?
        resp.Body.decode_json().as(body, {
          "events": (
            has(body.schema) && has(body.rows) ?
              body.rows.map(row,{
                "message": ({"row": row, "schema": body.schema}).encode_json(),
              })
            :
              []
          ),
          "cursor": {
            ?"last_timestamp": has(body.schema) && has(body.rows) && body.rows.size() > 0 ?
              // extracting timestamp of the last event from the event_info object.
              body.collate("schema.fields.name").zip(body.rows[body.rows.size()-1].f).as(last_record,
                has(last_record.event_info) ?
                  body.schema.fields.filter(field,
                    field.name == "event_info"
                  ).collate("fields.name").zip(last_record.collate("event_info.v.f.v"))
                :
                  {}
              ).as(last_event_info,
                has(last_event_info.timestamp_usec) ?
                  (
                    (
                      has(state.?cursor.last_timestamp) &&
                      last_event_info.timestamp_usec < state.cursor.last_timestamp
                    ) ?
                      optional.of(state.cursor.last_timestamp)
                    :
                      optional.of(last_event_info.timestamp_usec)
                  )
                :
                  state.?cursor.last_timestamp
              )
            :
              state.?cursor.last_timestamp
          },
          "next": {
            ?"page_token": body.?pageToken,
            // reset job ID if its last page.
            ?"job_id": has(body.pageToken) ? optional.of(state.next.job_id) : optional.none(),
          },
          "want_more": has(body.pageToken),
        })
      :
        {
          "events": {
            "error": {
              "code": string(resp.StatusCode),
              "id": string(resp.Status),
              "message": "GET " + state.url.trim_right("/") + "/bigquery/v2/projects/"+ state.project_id +"/queries/" + state.next.job_id + ": " + (
                size(resp.Body) != 0 ?
                  string(resp.Body)
                :
                  string(resp.Status) + ' (' + string(resp.StatusCode) + ')'
              ),
            },
          },
          "want_more": false,
          "next": {},
        }
      )
    :
      state
  ))
tags:
{{#if preserve_original_event}}
  - preserve_original_event
{{/if}}
{{#if preserve_duplicate_custom_fields}}
  - preserve_duplicate_custom_fields
{{/if}}
{{#each tags as |tag|}}
  - {{tag}}
{{/each}}
{{#contains "forwarded" tags}}
publisher_pipeline.disable_host: true
{{/contains}}
{{#if processors}}
processors:
{{processors}}
{{/if}}
