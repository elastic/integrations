---
description: Pipeline for processing sample logs
processors:
  - set:
      field: ecs.version
      value: '8.11.0'
  - set:
      copy_from: message
      field: event.original
      ignore_failure: true 
  - set:
      field: process.name
      copy_from: log.syslog.appname
      if: ctx.log?.syslog?.appname != null

  #
  # Extract the key=value pairs into nested objects
  #
  # The syslog header should have been stripped off, so we need to process the key=value data
  # We will be using painless here, because it is the best option we have.
  - script:
      tag: script_kv_parse
      description: Parse key/value pairs from message.
      lang: painless
      source: >-
        ctx["ss"] = new HashMap();

        def kvStart = 0;
        def kvSplit = 0;
        def kvEnd = 0;
        def inQuote = false;

        for (int i = 0, n = ctx["message"].length(); i < n; ++i) {
          char c = ctx["message"].charAt(i);
          if (c == (char)'"') {
            inQuote = !inQuote;
          }
          if (inQuote) {
            continue;
          }
          
          if (c == (char)'=') {
            kvSplit = i;
          }
          if (c == (char)' ' || (i == n - 1)) {
            if (kvStart != kvSplit) {
              def key = ctx["message"].substring(kvStart, kvSplit);
              def value = ctx["message"].substring(kvSplit + 1, i).replace("\"", "");
              ctx["ss"][key] = value;
            }

            kvStart = i + 1;
            kvSplit = i + 1;
          }
        }

  - grok:
      field: ss.tz
      patterns:
        - "(?:%{OFFSET:_temp_.tz_offset})(?:%{HOUR:_temp_.tz_hour}):?(?:%{MINUTE:_temp_.tz_minute})"
      pattern_definitions:
        OFFSET: "[+-]?"
  
  # rename some fields

  - set:
      field: "event.timezone"
      value: "{{{_temp_.tz_offset}}}{{{_temp_.tz_hour}}}:{{{_temp_.tz_minute}}}"
      if: ctx._temp_?.tz_hour != null

  - rename:
      field: "ss.msg"
      target_field: "event.log.msg"
      if: ctx.ss?.msg != null

  - date:
      field: ss.startime
      target_field: "event.created"
      formats:
        - "yyyy-MM-dd HH:mm:ss"
      timezone: "{{{event.timezone}}}"
      on_failure:
        - remove:
            field: event.created
            ignore_missing: true
        - append:
            field: error.message
            value: "{{{_ingest.on_failure_message}}}"

  # Set @timestamp to the time when the log indicates.
  - set:
      copy_from: event.created
      field: '@timestamp'
      on_failure:
        - append:
            field: error.message
            value: "{{{_ingest.on_failure_message}}}"

  - set:
      copy_from: "ss.fw"
      field: "host.name"
      if: ctx.ss?.fw != null

  - set:
      copy_from: "ss.logtype"
      field: "stormshield.logtype"
      if: ctx.ss?.logtype != null

  - pipeline:
      name: '{{ IngestPipeline "filterstat" }}'
      if: ctx.stormshield.logtype == 'filterstat'
  #
  # The syslog header should have been stripped off, so we need to process the key=value data
  # We will be using painless here, because it is the best option we have.
  - script:
      tag: expand_dynamic_fields
      description: Expands the EthernetXX field.
      lang: painless
      source: >-
        void expand_(Map mapper, String token, String fieldName, String base, def availableFields, boolean skipOriginal, boolean flatten) {
            def obj = new HashMap();

            if (! skipOriginal) {
                obj["original"] = fieldName;
            }
            String field = mapper[fieldName];
            String[] fields = field.splitOnToken(token);

            // availableFields and fields should have the same length
            for (int i = 0; i < availableFields.length; ++i) {
                obj[availableFields[i]] = fields[i];
            }
            if (flatten) {
                mapper[base] = obj;
            } else {
                if (! mapper.containsKey(base)) {
                    mapper[base] = new ArrayList();
                }
                mapper[base].add(obj);
            }
        }
        void handleMove(Map context, String namespace) {
            if (context.containsKey("ss")) {
                context["stormshield"]["logtype"] = context["ss"]["logtype"];
                context["ss"].remove("logtype");
            }

            context["stormshield"][namespace] = context["ss"];
            context.remove("ss");
        }
        void rename_field(Map mapper, String orig, String newName) {
            if (mapper.containsKey(orig)) {
                mapper[newName] = mapper[orig];
                mapper.remove(orig);
            }
        }
        void renameNumberedDynamicFields(Map mapper, String fieldBase, def availableFields) {
            for (int i = 0;; i++) {
                String baseName = fieldBase;
                String eth = baseName + Integer.toString(i);

                // if this does not exist, then we break
                if (! mapper.containsKey(eth)) {
                    break;
                }

                expand_(mapper, ",", eth, fieldBase, availableFields, /* skipOriginal = */ false, /* flatten = */ false);
                mapper.remove(eth);
            }
        }

        handleMove(ctx, ctx.ss.logtype);

        if (ctx.stormshield.logtype == "monitor") {
            def monitor = ctx.stormshield.monitor;
            rename_field(monitor, "CPU", "CPU_");
            rename_field(monitor, "ipsec", "ipsec_native");

            def availableFields = [
                "name",
                "incoming_throughput",
                "maximum_incoming_throughput",
                "outgoing_throughput",
                "maximum_outgoing_throughput",
                "packets_accepted",
                "packets_blocked"
            ];
            def fields = [
                "Ethernet",
                "Vlan",
                "Qid",
                "Wifi",
                "sslvpn",
                "ipsec",
                "agg"
            ];
            def cpu = ["user_time", "kernel_time", "system_disruption"];
            expand_(monitor, ",", "CPU_", "CPU", cpu, /* skipOriginal = */ true, /* flatten = */ true);
            monitor.remove("CPU_");
            expand_(monitor, ",", "ipsec_native", "ipsec", availableFields, /* skipOriginal = */ true, /* flatten = */ false);
            monitor.remove("ipsec_native");
            monitor["ipsec"][0]["native"] = true;
            for (field_base in fields) {
                renameNumberedDynamicFields(monitor, field_base, availableFields);
            }
        } else if (ctx.stormshield.logtype == "count") {
          def mapper = ctx.stormshield.count;
          // RuleX:Y
          // Split the name
          // Rule, category, rule_number
          def toRemove = new ArrayList();
          for (def entry : ctx.stormshield.count.entrySet()) {
            // if this starts with "Rule"
            String key = entry.getKey();
            if (key.startsWith("Rule") && key.contains(":")){
              int colon = key.indexOf(":");
              String category = key.substring(4, colon);
              String rule_number = key.substring(colon + 1);

              if (! mapper.containsKey("Rule")) {
                mapper["Rule"] = new ArrayList();
              }
              def rule = new HashMap();
              rule["category"] = category;
              rule["rule_number"] = rule_number;
              rule["byte_count"] = entry.getValue();
              mapper["Rule"].add(rule);
              toRemove.add(key);
            }
          }

          for (def entry : toRemove) {
            if (mapper.containsKey(entry)) {
              mapper.remove(entry);
            }
          }
        }

  - remove:
      field: _temp_
      ignore_missing: true
      ignore_failure: true

  - remove:
      field: event.original
      if: ctx.tags == null || !(ctx.tags.contains('preserve_original_event'))
      ignore_failure: true
      ignore_missing: true
  - remove:
      field: message
      ignore_failure: true
      ignore_missing: true
on_failure:
- append:
    field: error.message
    value: 'Processor {{{_ingest.on_failure_processor_type}}} with tag {{{_ingest.on_failure_processor_tag}}} in pipeline {{{_ingest.pipeline}}} failed with message: {{{_ingest.on_failure_message}}}'
- set:
    field: event.kind
    value: pipeline_error