---
description: Pipeline for processing sample logs
processors:
  - set:
      field: ecs.version
      value: '8.11.0'
  - set:
      copy_from: message
      field: event.original
      ignore_failure: true 
  - set:
      field: process.name
      copy_from: log.syslog.appname
      if: ctx.log?.syslog?.appname != null

  #
  # Extract the key=value pairs into nested objects
  #
  # The syslog header should have been stripped off, so we need to process the key=value data
  # We will be using painless here, because it is the best option we have.
  - script:
      tag: script_kv_parse
      description: Parse key/value pairs from message.
      lang: painless
      source: >-
        ctx["stormshield"] = new HashMap();

        def kvStart = 0;
        def kvSplit = 0;
        def kvEnd = 0;
        def inQuote = false;

        for (int i = 0, n = ctx["message"].length(); i < n; ++i) {
          char c = ctx["message"].charAt(i);
          if (c == (char)'"') {
            inQuote = !inQuote;
          }
          if (inQuote) {
            continue;
          }
          
          if (c == (char)'=') {
            kvSplit = i;
          }
          if (c == (char)' ' || (i == n - 1)) {
            if (kvStart != kvSplit) {
              def key = ctx["message"].substring(kvStart, kvSplit);
              def value = ctx["message"].substring(kvSplit + 1, i).replace("\"", "");
              ctx["stormshield"][key] = value;
            }

            kvStart = i + 1;
            kvSplit = i + 1;
          }
        }

  - grok:
      field: stormshield.tz
      patterns:
        - "(?:%{OFFSET:_temp_.tz_offset})(?:%{HOUR:_temp_.tz_hour}):?(?:%{MINUTE:_temp_.tz_minute})"
      pattern_definitions:
        OFFSET: "[+-]?"
  
  # rename some fields

  - set:
      field: "event.timezone"
      value: "{{{_temp_.tz_offset}}}{{{_temp_.tz_hour}}}:{{{_temp_.tz_minute}}}"
      if: ctx._temp_?.tz_hour != null

  - rename:
      field: "stormshield.msg"
      target_field: "event.log.msg"
      if: ctx.stormshield?.msg != null

  - date:
      field: stormshield.startime
      target_field: "event.created"
      formats:
        - "yyyy-MM-dd HH:mm:ss"
      timezone: "{{{event.timezone}}}"
      on_failure:
        - remove:
            field: event.created
            ignore_missing: true
        - append:
            field: error.message
            value: "{{{_ingest.on_failure_message}}}"

  # Set @timestamp to the time when the log indicates.
  - set:
      copy_from: event.created
      field: '@timestamp'
      on_failure:
        - append:
            field: error.message
            value: "{{{_ingest.on_failure_message}}}"

  - pipeline:
      name: '{{ IngestPipeline "filterstat" }}'
      if: ctx.stormshield.logtype == 'filterstat'

  - pipeline:
      name: '{{ IngestPipeline "monitor" }}'
      if: ctx.stormshield.logtype == 'monitor'

  - pipeline:
      name: '{{ IngestPipeline "count" }}'
      if: ctx.stormshield.logtype == 'count'

  #########################################################
  # Now rename things to ECS

  - rename:
      field: stormshield.dstcontinent
      target_field: destination.geo.continent_code
      if: ctx.stormshield?.dstcontinent != null

  - rename:
      field: stormshield.dstcountry
      target_field: destination.geo.country_iso_code
      if: ctx.stormshield?.dstcountry != null

  - rename:
      field: stormshield.dstname
      target_field: destination.domain
      if: ctx.stormshield?.dstname != null

  - convert:
      field: stormshield.dst
      target_field: destination.ip
      type: ip
      if: ctx.stormshield?.dst != null && ctx.destination?.ip == null

  - remove:
      field: stormshield.dst
      if: ctx.stormshield?.dst != null

  - rename:
      field: stormshield.dstmac
      target_field: destination.mac
      if: ctx.stormshield?.dstmac != null && ctx.destination?.mac == null

  - convert:
      field: stormshield.dstport
      target_field: destination.port
      type: long
      if: ctx.stormshield?.dstport != null && ctx.destination?.port == null

  - remove:
      field: stormshield.dstport
      if: ctx.stormshield?.dstport != null

  - rename:
      field: stormshield.srccontinent
      target_field: source.geo.continent_code
      if: ctx.stormshield?.srccontinent != null

  - rename:
      field: stormshield.srccountry
      target_field: source.geo.country_iso_code
      if: ctx.stormshield?.srccountry != null

  - convert:
      field: stormshield.src
      target_field: source.ip
      type: ip
      if: ctx.stormshield?.src != null && ctx.source?.ip == null

  - remove:
      field: stormshield.src
      if: ctx.stormshield?.src != null

  - rename:
      field: stormshield.srcmac
      target_field: source.mac
      if: ctx.stormshield?.srcmac != null && ctx.source?.mac == null

  - convert:
      field: stormshield.srcport
      target_field: source.port
      type: long
      if: ctx.stormshield?.srcport != null && ctx.source?.port == null

  - remove:
      field: stormshield.srcport
      if: ctx.stormshield?.srcport != null

  - remove:
      field: _temp_
      ignore_missing: true
      ignore_failure: true

  - remove:
      field: event.original
      if: ctx.tags == null || !(ctx.tags.contains('preserve_original_event'))
      ignore_failure: true
      ignore_missing: true
  - remove:
      field: message
      ignore_failure: true
      ignore_missing: true
on_failure:
- append:
    field: error.message
    value: 'Processor {{{_ingest.on_failure_processor_type}}} with tag {{{_ingest.on_failure_processor_tag}}} in pipeline {{{_ingest.pipeline}}} failed with message: {{{_ingest.on_failure_message}}}'
- set:
    field: event.kind
    value: pipeline_error