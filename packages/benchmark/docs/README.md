# Benchmark

The Benchmark integration allows you to send synthetic events from `elastic-agent` to the configured output.

Use the Benchmark integration to test how fast events can be sent from `elastic-agent`. Then visualize that data in Kibana.  This can be useful when tracking down performance problems or when you want to test connectivity.

## Data streams

The Benchmark integration collects one type of data streams: logs.

**Logs** are the synthetic events

Log data streams collected by the benchmark integration include `benchmark_output.generic`. See more details in the [Logs](#logs-reference). -->


## Requirements

You need Elasticsearch for storing and searching your data and Kibana for visualizing and managing it.
You can use our hosted Elasticsearch Service on Elastic Cloud, which is recommended, or self-manage the Elastic Stack on your own hardware.

## Setup

For step-by-step instructions on how to set up an integration, see the
[Getting started](https://www.elastic.co/guide/en/welcome-to-elastic/current/getting-started-observability.html) guide.

Options include:

*** Message text

This is the what will appear in the `message` field of each event

*** Threads

This is the number of go routines that will be generating synthetic events.  Usually one thread is enough, but for large machines with multiple CPUs and outputs with multiple `worker` defined, it may be necessary to increase this.

*** Number of Events

Use this option if you just want to send a certain number of events and then stop.

*** Events per second

Use this option if you want to send a steady rate of events per second.  Leaving both `Number of events` and `Events per second` empty means to send as fast as possible until you remove the integration.


### benchmark_output.generic

The `benchmark_output.generic` data stream holds the synthetic events generated by this integration.

**Exported fields**

| Field | Description | Type |
|---|---|---|
| @timestamp | Event timestamp. | date |
| data_stream.dataset | Data stream dataset. | constant_keyword |
| data_stream.namespace | Data stream namespace. | constant_keyword |
| data_stream.type | Data stream type. | constant_keyword |
| ecs.version | ECS version this event conforms to. `ecs.version` is a required field and must exist in all events. When querying across multiple indices -- which may conform to slightly different ECS versions -- this field lets integrations adjust to the schema version of the events. | keyword |
| event.dataset | Event dataset | constant_keyword |
| event.module | Event module | constant_keyword |
| event.original | Raw text message of entire event. Used to demonstrate log integrity or where the full log message (before splitting it up in multiple parts) may be required, e.g. for reindex. This field is not indexed and doc_values are disabled. It cannot be searched, but it can be retrieved from `_source`. If users wish to override this and index this field, please see `Field data types` in the `Elasticsearch Reference`. | keyword |
| filename | virtual filename of event.  range is 0 to 2^64. | keyword |
| input.type | Type of Filebeat input. | keyword |
| line | virtual line number of event.  range is 0 to 2^64. | keyword |
| log.level | Original log level of the log event. If the source of the event provides a log level or textual severity, this is the one that goes in `log.level`. If your source doesn't specify one, you may put your event transport's severity here (e.g. Syslog severity). Some examples are `warn`, `err`, `i`, `informational`. | keyword |
| message | For log events the message field contains the log message, optimized for viewing in a log viewer. For structured logs without an original message field, other fields can be concatenated to form a human-readable summary of the event. If multiple messages exist, they can be combined into one message. | match_only_text |
| tags | User defined tags | keyword |
| thread | thread that generated the event. | keyword |

