- name: azure.open_ai
  type: group
  fields:
    - name: requests.total
      type: long
      metric_type: gauge
      description: Number of calls made to the Azure OpenAI API over a period of time.
    - name: generated_tokens.total
      type: long
      metric_type: gauge
      description: Number of tokens generated (output) from an OpenAI model.
    - name: active_tokens.total
      type: long
      metric_type: gauge
      description: Total tokens minus cached tokens over a period of time.
    - name: processed_prompt_tokens.total
      type: long
      metric_type: gauge
      description: Number of prompt tokens processed (input) on an OpenAI model.
    - name: token_transaction.total
      type: long
      metric_type: gauge
      description: Number of inference tokens processed on an OpenAI model.
    - name: fine_tuned_training_hours.total
      type: float
      metric_type: gauge
      description: Number of Training Hours Processed on an OpenAI FineTuned Model.
    - name: time_to_response.avg
      type: float
      metric_type: gauge
      description: Recommended latency (responsiveness) measure for streaming requests. Applies to PTU and PTU-managed deployments. Calculated as time taken for the first response to appear after a user sends a prompt, as measured by the API gateway. This number increases as the prompt size increases and/or cache hit size reduces.
    - name: context_tokens_cache_match_rate.avg
      type: float
      metric_type: gauge
      description: Percentage of the prompt tokens hit the cache (Avaiable for PTU-managed).
    - name: provisioned_managed_utilization_v2.avg
      type: float
      metric_type: gauge
      unit: percent
      description: Utilization % for a provisoned-managed deployment, calculated as (PTUs consumed / PTUs deployed) x 100. When utilization is greater than or equal to 100%, calls are throttled and error code 429 returned.
